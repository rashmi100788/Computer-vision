{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Face recognition - Questions - Project - CV - AIML Online.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"VvWl3ebqzCc1","colab_type":"text"},"source":["# Instructions\n","- Some parts of the code are already done for you\n","- You need to execute all the cells\n","- You need to add the code where ever you see `\"#### Add your code here ####\"`\n","- Marks are mentioned along with the cells"]},{"cell_type":"markdown","metadata":{"id":"NgR0j5310qqC","colab_type":"text"},"source":["# Face recognition\n","Task is to recognize a faces"]},{"cell_type":"markdown","metadata":{"id":"X_f3HHLmJIuT","colab_type":"text"},"source":["### Dataset\n","**Aligned Face Dataset from Pinterest**\n","\n","This dataset contains 10.770 images for 100 people. All images are taken from 'Pinterest' and      aligned using dlib library."]},{"cell_type":"code","metadata":{"id":"Fn8OlNXgMarq","colab_type":"code","colab":{}},"source":["%tensorflow_version 2.x"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"aV3hpS9ciSFr","colab_type":"code","colab":{}},"source":["import tensorflow\n","tensorflow.__version__"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"CjRTlPkp1LC2","colab_type":"text"},"source":["#### Mount Google drive if you are using google colab\n","- We recommend using Google Colab as you can face memory issues and longer runtimes while running on local"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"sBWMoTJ9cf3Z","colab":{}},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"sO9mgMmp13sI","colab_type":"text"},"source":["#### Change current working directory to project folder (1 mark)"]},{"cell_type":"code","metadata":{"id":"TddMnf4D1-59","colab_type":"code","colab":{}},"source":["#### Add your code here ####"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"CBB_OncAQ8h_","colab_type":"text"},"source":["### Extract the zip file (2 marks)\n","- Extract Aligned Face Dataset from Pinterest.zip"]},{"cell_type":"code","metadata":{"id":"CI5uhBunLEZ9","colab_type":"code","colab":{}},"source":["#### Add your code here ####"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"oesXJD9ySB6w","colab_type":"text"},"source":["### Function to load images\n","- Define a function to load the images from the extracted folder and map each image with person id \n"]},{"cell_type":"code","metadata":{"id":"4Q7TS19vVbGb","colab_type":"code","colab":{}},"source":["import numpy as np\n","import os\n","\n","class IdentityMetadata():\n","    def __init__(self, base, name, file):\n","        # print(base, name, file)\n","        # dataset base directory\n","        self.base = base\n","        # identity name\n","        self.name = name\n","        # image file name\n","        self.file = file\n","\n","    def __repr__(self):\n","        return self.image_path()\n","\n","    def image_path(self):\n","        return os.path.join(self.base, self.name, self.file) \n","    \n","def load_metadata(path):\n","    metadata = []\n","    for i in os.listdir(path):\n","        for f in os.listdir(os.path.join(path, i)):\n","            # Check file extension. Allow only jpg/jpeg' files.\n","            ext = os.path.splitext(f)[1]\n","            if ext == '.jpg' or ext == '.jpeg':\n","                metadata.append(IdentityMetadata(path, i, f))\n","    return np.array(metadata)\n","\n","# metadata = load_metadata('images')\n","metadata = load_metadata('PINS')"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"nG1Vzl3MPebA","colab_type":"text"},"source":["### Define function to load image\n","- Define a function to load image from the metadata"]},{"cell_type":"code","metadata":{"id":"ape5WxvVWKOe","colab_type":"code","colab":{}},"source":["import cv2\n","def load_image(path):\n","    img = cv2.imread(path, 1)\n","    # OpenCV loads images with color channels\n","    # in BGR order. So we need to reverse them\n","    return img[...,::-1]"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"DYm-aYUDRANv","colab_type":"text"},"source":["#### Load a sample image (2 marks)\n","- Load one image using the function \"load_image\""]},{"cell_type":"code","metadata":{"id":"ptDNq8noWK89","colab_type":"code","colab":{}},"source":["#### Add your code here ####"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"yg0olr-8Xbqw","colab_type":"text"},"source":["### VGG Face model\n","- Here we are giving you the predefined model for VGG face"]},{"cell_type":"code","metadata":{"id":"hh0Pz6acuaDP","colab_type":"code","colab":{}},"source":["from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import ZeroPadding2D, Convolution2D, MaxPooling2D, Dropout, Flatten, Activation\n","\n","def vgg_face():\t\n","    model = Sequential()\n","    model.add(ZeroPadding2D((1,1),input_shape=(224,224, 3)))\n","    model.add(Convolution2D(64, (3, 3), activation='relu'))\n","    model.add(ZeroPadding2D((1,1)))\n","    model.add(Convolution2D(64, (3, 3), activation='relu'))\n","    model.add(MaxPooling2D((2,2), strides=(2,2)))\n","    \n","    model.add(ZeroPadding2D((1,1)))\n","    model.add(Convolution2D(128, (3, 3), activation='relu'))\n","    model.add(ZeroPadding2D((1,1)))\n","    model.add(Convolution2D(128, (3, 3), activation='relu'))\n","    model.add(MaxPooling2D((2,2), strides=(2,2)))\n","    \n","    model.add(ZeroPadding2D((1,1)))\n","    model.add(Convolution2D(256, (3, 3), activation='relu'))\n","    model.add(ZeroPadding2D((1,1)))\n","    model.add(Convolution2D(256, (3, 3), activation='relu'))\n","    model.add(ZeroPadding2D((1,1)))\n","    model.add(Convolution2D(256, (3, 3), activation='relu'))\n","    model.add(MaxPooling2D((2,2), strides=(2,2)))\n","    \n","    model.add(ZeroPadding2D((1,1)))\n","    model.add(Convolution2D(512, (3, 3), activation='relu'))\n","    model.add(ZeroPadding2D((1,1)))\n","    model.add(Convolution2D(512, (3, 3), activation='relu'))\n","    model.add(ZeroPadding2D((1,1)))\n","    model.add(Convolution2D(512, (3, 3), activation='relu'))\n","    model.add(MaxPooling2D((2,2), strides=(2,2)))\n","    \n","    model.add(ZeroPadding2D((1,1)))\n","    model.add(Convolution2D(512, (3, 3), activation='relu'))\n","    model.add(ZeroPadding2D((1,1)))\n","    model.add(Convolution2D(512, (3, 3), activation='relu'))\n","    model.add(ZeroPadding2D((1,1)))\n","    model.add(Convolution2D(512, (3, 3), activation='relu'))\n","    model.add(MaxPooling2D((2,2), strides=(2,2)))\n","    \n","    model.add(Convolution2D(4096, (7, 7), activation='relu'))\n","    model.add(Dropout(0.5))\n","    model.add(Convolution2D(4096, (1, 1), activation='relu'))\n","    model.add(Dropout(0.5))\n","    model.add(Convolution2D(2622, (1, 1)))\n","    model.add(Flatten())\n","    model.add(Activation('softmax'))\n","    return model"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"j2JhG4NOe7vd","colab_type":"text"},"source":["#### Load the model (2 marks)\n","- Load the model defined above\n","- Then load the given weight file named \"vgg_face_weights.h5\""]},{"cell_type":"code","metadata":{"id":"zAa3OASPvKac","colab_type":"code","colab":{}},"source":["model = #### Add your code here ####\n","#### Add your code here ####"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"mStdpxzAf7y5","colab_type":"text"},"source":["### Get vgg_face_descriptor"]},{"cell_type":"code","metadata":{"id":"j9IQ9hcSwO9k","colab_type":"code","colab":{}},"source":["from tensorflow.keras.models import Model\n","vgg_face_descriptor = Model(inputs=model.layers[0].input, outputs=model.layers[-2].output)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"LkBQRL_sd2U8","colab_type":"text"},"source":["### Generate embeddings for each image in the dataset\n","- Given below is an example to load the first image in the metadata and get its embedding vector from the pre-trained model. "]},{"cell_type":"code","metadata":{"id":"B2yd69OydBAq","colab_type":"code","colab":{}},"source":["# Get embedding vector for first image in the metadata using the pre-trained model\n","\n","img_path = metadata[0].image_path()\n","img = load_image(img_path)\n","\n","# Normalising pixel values from [0-255] to [0-1]: scale RGB values to interval [0,1]\n","img = (img / 255.).astype(np.float32)\n","\n","img = cv2.resize(img, dsize = (224,224))\n","print(img.shape)\n","\n","# Obtain embedding vector for an image\n","# Get the embedding vector for the above image using vgg_face_descriptor model and print the shape \n","\n","embedding_vector = vgg_face_descriptor.predict(np.expand_dims(img, axis=0))[0]\n","print(embedding_vector.shape)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"plHvUTytcTGo","colab_type":"text"},"source":["### Generate embeddings for all images (5 marks)\n","- Write code to iterate through metadata and create embeddings for each image using `vgg_face_descriptor.predict()` and store in a list with name `embeddings`\n","\n","- If there is any error in reading any image in the dataset, fill the emebdding vector of that image with 2622-zeroes as the final embedding from the model is of length 2622."]},{"cell_type":"code","metadata":{"id":"yY9ykxtueY4k","colab_type":"code","colab":{}},"source":["#### Add your code here ####"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"4hb3XSDsfTMG","colab_type":"text"},"source":["### Function to calculate distance between given 2 pairs of images.\n","\n","- Consider distance metric as \"Squared L2 distance\"\n","- Squared l2 distance between 2 points (x1, y1) and (x2, y2) = (x1-x2)^2 + (y1-y2)^2"]},{"cell_type":"code","metadata":{"id":"0sNnRtt-U7aU","colab_type":"code","colab":{}},"source":["def distance(emb1, emb2):\n","    return np.sum(np.square(emb1 - emb2))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"JwVRkeoNUyUw","colab_type":"text"},"source":["#### Plot images and get distance between the pairs given below\n","- 2, 3 and 2, 180\n","- 30, 31 and 30, 100\n","- 70, 72 and 70, 115"]},{"cell_type":"code","metadata":{"id":"nDVLED10eboB","colab_type":"code","colab":{}},"source":["import matplotlib.pyplot as plt\n","\n","def show_pair(idx1, idx2):\n","    plt.figure(figsize=(8,3))\n","    plt.suptitle(f'Distance = {distance(embeddings[idx1], embeddings[idx2]):.2f}')\n","    plt.subplot(121)\n","    plt.imshow(load_image(metadata[idx1].image_path()))\n","    plt.subplot(122)\n","    plt.imshow(load_image(metadata[idx2].image_path()));    \n","\n","show_pair(2, 3)\n","show_pair(2, 180)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-G2iDeWKYMae","colab_type":"text"},"source":["### Create train and test sets (5 marks)\n","- Create X_train, X_test and y_train, y_test\n","- Use train_idx to seperate out training features and labels\n","- Use test_idx to seperate out testing features and labels"]},{"cell_type":"code","metadata":{"id":"OThdBDPxYkd4","colab_type":"code","colab":{}},"source":["train_idx = np.arange(metadata.shape[0]) % 9 != 0\n","test_idx = np.arange(metadata.shape[0]) % 9 == 0\n","\n","#### Add your code here ####"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"DlYYwGQxXVwf","colab_type":"text"},"source":["### Encode the Labels (3 marks)\n","- Encode the targets\n","- Use LabelEncoder"]},{"cell_type":"code","metadata":{"id":"8GOQrjqeX2LZ","colab_type":"code","colab":{}},"source":["from sklearn.preprocessing import LabelEncoder\n","\n","#### Add your code here ####"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"o9CylOWOa4xM","colab_type":"text"},"source":["### Standardize the feature values (3 marks)\n","- Scale the features using StandardScaler"]},{"cell_type":"code","metadata":{"id":"H7pUV0oYbLrR","colab_type":"code","colab":{}},"source":["# Standarize features\n","from sklearn.preprocessing import StandardScaler\n","\n","#### Add your code here ####"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"i2QukHGXbb6d","colab_type":"text"},"source":["### Reduce dimensions using PCA (3 marks)\n","- Reduce feature dimensions using Principal Component Analysis"]},{"cell_type":"code","metadata":{"id":"dVj1SSEebtG8","colab_type":"code","colab":{}},"source":["from sklearn.decomposition import PCA\n","\n","#### Add your code here ####"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"SzCsmZg8chW4","colab_type":"text"},"source":["### Build a Classifier (3 marks)\n","- Use SVM Classifier to predict the person in the given image\n","- Fit the classifier and print the score"]},{"cell_type":"code","metadata":{"id":"MnBv9Ks0cwtA","colab_type":"code","colab":{}},"source":["from sklearn.svm import SVC\n","\n","#### Add your code here ####"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"JGz1G8e3dUl5","colab_type":"text"},"source":["### Test results (1 mark)\n","- Take 10th image from test set and plot the image\n","- Report to which person(folder name in dataset) the image belongs to"]},{"cell_type":"code","metadata":{"id":"4zD_f8Sudeiw","colab_type":"code","colab":{}},"source":["import warnings\n","# Suppress LabelEncoder warning\n","warnings.filterwarnings('ignore')\n","\n","example_idx = 10\n","\n","example_image = load_image(metadata[test_idx][example_idx].image_path())\n","example_prediction = #### Add your code here ####\n","example_identity = encoder.inverse_transform(example_prediction)[0]\n","\n","plt.imshow(example_image)\n","plt.title(f'Identified as {example_identity}');"],"execution_count":0,"outputs":[]}]}